{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  <center> Speech Emotion Recognition (SER Superset) <center>\n##   <center> (With Cross Validation) <center>","metadata":{}},{"cell_type":"markdown","source":"### Importing Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-05-17T16:41:01.068077Z","iopub.execute_input":"2023-05-17T16:41:01.068592Z","iopub.status.idle":"2023-05-17T16:41:10.308258Z","shell.execute_reply.started":"2023-05-17T16:41:01.068546Z","shell.execute_reply":"2023-05-17T16:41:10.307233Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Data Preparation\n* As we are working with four different datasets, so we will be creating a dataframe storing all emotions of the data in dataframe with their paths.\n* We will use this dataframe to extract features for our model training.","metadata":{}},{"cell_type":"markdown","source":"### Read Datasets","metadata":{}},{"cell_type":"code","source":"# Paths for data.\nRavdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nCrema = \"/kaggle/input/cremad/AudioWAV/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:10.309826Z","iopub.execute_input":"2023-05-17T16:41:10.310477Z","iopub.status.idle":"2023-05-17T16:41:10.315867Z","shell.execute_reply.started":"2023-05-17T16:41:10.310443Z","shell.execute_reply":"2023-05-17T16:41:10.314686Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"##  <center> 1. Ravdess Dataframe <center>\nHere is the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).","metadata":{}},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\n\nfile_emotion = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(Ravdess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nRavdess_df = pd.concat([emotion_df, path_df], axis=1)\n\n# changing integers to actual emotions.\nRavdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df['Actor'] = Ravdess_df['Path'].str.split('-').str[-1].str.split('.').str[0]\nRavdess_df['Actor'] = Ravdess_df['Actor'].apply(lambda x: int(x))\nRavdess_df['Gender'] = np.where(Ravdess_df['Actor'].astype(int) % 2 == 1, 'Male', 'Female')\n\n\npd.set_option('display.max_colwidth', None)\nRavdess_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:10.317059Z","iopub.execute_input":"2023-05-17T16:41:10.317455Z","iopub.status.idle":"2023-05-17T16:41:10.627306Z","shell.execute_reply.started":"2023-05-17T16:41:10.317422Z","shell.execute_reply":"2023-05-17T16:41:10.626046Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Emotions  \\\n0  surprise   \n1   neutral   \n2   disgust   \n3   disgust   \n4   neutral   \n\n                                                                                                       Path  \\\n0  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-08-01-01-01-02.wav   \n1  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-01-01-02.wav   \n2  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-02-01-02-02.wav   \n3  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-01-01-02-02.wav   \n4  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-02-01-02.wav   \n\n   Actor  Gender  \n0      2  Female  \n1      2  Female  \n2      2  Female  \n3      2  Female  \n4      2  Female  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotions</th>\n      <th>Path</th>\n      <th>Actor</th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>surprise</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-08-01-01-01-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-01-01-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-02-01-02-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-01-01-02-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-02-01-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## <center>2. Crema DataFrame</center>","metadata":{}},{"cell_type":"code","source":"crema_directory_list = os.listdir(Crema)\n\nfile_emotion = []\nfile_path = []\n\nfor file in crema_directory_list:\n    # storing file paths\n    file_path.append(Crema + file)\n    # storing file emotions\n    part=file.split('_')\n    if part[2] == 'SAD':\n        file_emotion.append('sad')\n    elif part[2] == 'ANG':\n        file_emotion.append('angry')\n    elif part[2] == 'DIS':\n        file_emotion.append('disgust')\n    elif part[2] == 'FEA':\n        file_emotion.append('fear')\n    elif part[2] == 'HAP':\n        file_emotion.append('happy')\n    elif part[2] == 'NEU':\n        file_emotion.append('neutral')\n    else:\n        file_emotion.append('Unknown')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nCrema_df = pd.concat([emotion_df, path_df], axis=1)\ncrema_gender = pd.read_csv(\"/kaggle/input/gender-for-crema/actor-genderid - actor-genderid.csv\")\ncrema_gender = crema_gender.rename(columns={'Sex': 'Gender'})\ncrema_gender = crema_gender.drop(['Age','Race','Ethnicity'],axis =1)\ncrema_gender.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:10.629942Z","iopub.execute_input":"2023-05-17T16:41:10.630275Z","iopub.status.idle":"2023-05-17T16:41:10.828510Z","shell.execute_reply.started":"2023-05-17T16:41:10.630246Z","shell.execute_reply":"2023-05-17T16:41:10.827540Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   ActorID  Gender\n0     1001    Male\n1     1002  Female\n2     1003  Female\n3     1004  Female\n4     1005    Male","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ActorID</th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1002</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1003</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1004</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1005</td>\n      <td>Male</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"Crema_df['match_actorID'] = Crema_df['Path'].str.extract(r'(\\d+)')\n\n\ncrema_gender['match_actorID'] = crema_gender['ActorID'].astype(str)\n\n# Perform the merge based on the common value\nmerged_df_crema = pd.merge(Crema_df, crema_gender, on='match_actorID',how = 'left')\n\n\nmerged_df_crema  = merged_df_crema .drop(['match_actorID'],axis =1)\nmerged_df_crema  = merged_df_crema .rename(columns={'Gender_y': 'Gender'})\nmerged_df_crema  = merged_df_crema.rename(columns={'ActorID': 'Actor'})","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:10.830000Z","iopub.execute_input":"2023-05-17T16:41:10.830317Z","iopub.status.idle":"2023-05-17T16:41:10.880364Z","shell.execute_reply.started":"2023-05-17T16:41:10.830289Z","shell.execute_reply":"2023-05-17T16:41:10.879326Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"actor_list = list(merged_df_crema['Actor'].unique())\nactor_list.sort()\nactor_dict = {}\ncorrected_actor_id = list(range(31,122))\nfor i in range(len(actor_list)):\n    actor_dict[actor_list[i]] = corrected_actor_id[i]\nmerged_df_crema['Actor'] = merged_df_crema['Actor'].replace(actor_dict)\nmerged_df_crema.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:10.881680Z","iopub.execute_input":"2023-05-17T16:41:10.882032Z","iopub.status.idle":"2023-05-17T16:41:10.902747Z","shell.execute_reply.started":"2023-05-17T16:41:10.882002Z","shell.execute_reply":"2023-05-17T16:41:10.901891Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"  Emotions                                               Path  Actor  Gender\n0  disgust  /kaggle/input/cremad/AudioWAV/1028_TSI_DIS_XX.wav     58  Female\n1    happy  /kaggle/input/cremad/AudioWAV/1075_IEO_HAP_LO.wav    105  Female\n2    happy  /kaggle/input/cremad/AudioWAV/1084_ITS_HAP_XX.wav    114  Female\n3  disgust  /kaggle/input/cremad/AudioWAV/1067_IWW_DIS_XX.wav     97    Male\n4  disgust  /kaggle/input/cremad/AudioWAV/1066_TIE_DIS_XX.wav     96    Male","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotions</th>\n      <th>Path</th>\n      <th>Actor</th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>disgust</td>\n      <td>/kaggle/input/cremad/AudioWAV/1028_TSI_DIS_XX.wav</td>\n      <td>58</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>happy</td>\n      <td>/kaggle/input/cremad/AudioWAV/1075_IEO_HAP_LO.wav</td>\n      <td>105</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>happy</td>\n      <td>/kaggle/input/cremad/AudioWAV/1084_ITS_HAP_XX.wav</td>\n      <td>114</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/kaggle/input/cremad/AudioWAV/1067_IWW_DIS_XX.wav</td>\n      <td>97</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>disgust</td>\n      <td>/kaggle/input/cremad/AudioWAV/1066_TIE_DIS_XX.wav</td>\n      <td>96</td>\n      <td>Male</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"##  <center> 3. TESS dataset <center>","metadata":{}},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nfile_emotion = []\nfile_path = []\n\nfor dir in tess_directory_list:\n    directories = os.listdir(Tess + dir)\n    for file in directories:\n        part = file.split('.')[0]\n        part = part.split('_')[2]\n        if part=='ps':\n            file_emotion.append('surprise')\n        else:\n            file_emotion.append(part)\n        file_path.append(Tess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nTess_df = pd.concat([emotion_df, path_df], axis=1)\nTess_df['Gender'] = 'Female'\npd.set_option('display.max_colwidth', None)\nTess_df['Actor'] = Tess_df['Path'].str.extract(r'/(OAF|YAF)_')\n\n# Map the extracted values to the corresponding actor numbers\nactor_mapping = {'OAF': 25, 'YAF': 26}\nTess_df['Actor'] = Tess_df['Actor'].map(actor_mapping)\nTess_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:10.904187Z","iopub.execute_input":"2023-05-17T16:41:10.905186Z","iopub.status.idle":"2023-05-17T16:41:11.552489Z","shell.execute_reply.started":"2023-05-17T16:41:10.905153Z","shell.execute_reply":"2023-05-17T16:41:11.551275Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  Emotions  \\\n0     fear   \n1     fear   \n2     fear   \n3     fear   \n4     fear   \n\n                                                                                                                                                         Path  \\\n0    /kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_home_fear.wav   \n1   /kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_youth_fear.wav   \n2    /kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_near_fear.wav   \n3  /kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_search_fear.wav   \n4    /kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_pick_fear.wav   \n\n   Gender  Actor  \n0  Female     26  \n1  Female     26  \n2  Female     26  \n3  Female     26  \n4  Female     26  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotions</th>\n      <th>Path</th>\n      <th>Gender</th>\n      <th>Actor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_home_fear.wav</td>\n      <td>Female</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_youth_fear.wav</td>\n      <td>Female</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_near_fear.wav</td>\n      <td>Female</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_search_fear.wav</td>\n      <td>Female</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fear</td>\n      <td>/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_fear/YAF_pick_fear.wav</td>\n      <td>Female</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"##  <center> 4. SAVEE dataset <center>\nThe audio files in this dataset are named in such a way that the prefix letters describes the emotion classes as follows:\n\n* 'a' = 'anger'\n* 'd' = 'disgust'\n* 'f' = 'fear'\n* 'h' = 'happiness'\n* 'n' = 'neutral'\n* 'sa' = 'sadness'\n* 'su' = 'surprise'","metadata":{}},{"cell_type":"code","source":"savee_directory_list = os.listdir(Savee)\n\nfile_emotion = []\nfile_path = []\n\nfor file in savee_directory_list:\n    file_path.append(Savee + file)\n    part = file.split('_')[1]\n    ele = part[:-6]\n    if ele=='a':\n        file_emotion.append('angry')\n    elif ele=='d':\n        file_emotion.append('disgust')\n    elif ele=='f':\n        file_emotion.append('fear')\n    elif ele=='h':\n        file_emotion.append('happy')\n    elif ele=='n':\n        file_emotion.append('neutral')\n    elif ele=='sa':\n        file_emotion.append('sad')\n    else:\n        file_emotion.append('surprise')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nSavee_df = pd.concat([emotion_df, path_df], axis=1)\nSavee_df['Gender'] = 'Male'\nSavee_df['Actor'] = Savee_df['Path'].str.extract(r'/(DC|JE|JK|KL)_')\n\n# Map the extracted values to the corresponding actor numbers\nactor_mapping = {'DC': 27, 'JE': 28, 'JK': 29, 'KL': 30}\nSavee_df['Actor'] = Savee_df['Actor'].map(actor_mapping)\nSavee_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:11.554061Z","iopub.execute_input":"2023-05-17T16:41:11.554386Z","iopub.status.idle":"2023-05-17T16:41:11.711623Z","shell.execute_reply.started":"2023-05-17T16:41:11.554358Z","shell.execute_reply":"2023-05-17T16:41:11.710895Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"  Emotions  \\\n0    happy   \n1     fear   \n2    happy   \n3  disgust   \n4    angry   \n\n                                                                      Path  \\\n0  /kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/JE_h09.wav   \n1  /kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/KL_f12.wav   \n2  /kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/DC_h03.wav   \n3  /kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/DC_d04.wav   \n4  /kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/KL_a14.wav   \n\n  Gender  Actor  \n0   Male     28  \n1   Male     30  \n2   Male     27  \n3   Male     27  \n4   Male     30  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotions</th>\n      <th>Path</th>\n      <th>Gender</th>\n      <th>Actor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>happy</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/JE_h09.wav</td>\n      <td>Male</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fear</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/KL_f12.wav</td>\n      <td>Male</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>happy</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/DC_h03.wav</td>\n      <td>Male</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/DC_d04.wav</td>\n      <td>Male</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>angry</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/KL_a14.wav</td>\n      <td>Male</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# creating Dataframe using all the 4 dataframes we created so far.\ndata_path = pd.concat([Ravdess_df, merged_df_crema, Tess_df, Savee_df], axis = 0)\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:11.712634Z","iopub.execute_input":"2023-05-17T16:41:11.713517Z","iopub.status.idle":"2023-05-17T16:41:11.778579Z","shell.execute_reply.started":"2023-05-17T16:41:11.713481Z","shell.execute_reply":"2023-05-17T16:41:11.777477Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   Emotions  \\\n0  surprise   \n1   neutral   \n2   disgust   \n3   disgust   \n4   neutral   \n\n                                                                                                       Path  \\\n0  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-08-01-01-01-02.wav   \n1  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-01-01-02.wav   \n2  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-02-01-02-02.wav   \n3  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-01-01-02-02.wav   \n4  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-02-01-02.wav   \n\n   Actor  Gender  \n0      2  Female  \n1      2  Female  \n2      2  Female  \n3      2  Female  \n4      2  Female  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotions</th>\n      <th>Path</th>\n      <th>Actor</th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>surprise</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-08-01-01-01-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-01-01-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-02-01-02-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-01-01-02-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-02-01-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_path.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:11.780978Z","iopub.execute_input":"2023-05-17T16:41:11.781284Z","iopub.status.idle":"2023-05-17T16:41:11.786930Z","shell.execute_reply.started":"2023-05-17T16:41:11.781257Z","shell.execute_reply":"2023-05-17T16:41:11.785868Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(12162, 4)"},"metadata":{}}]},{"cell_type":"code","source":"gender_distribution = data_path['Gender'].value_counts()\ngender_distribution","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:11.788317Z","iopub.execute_input":"2023-05-17T16:41:11.788609Z","iopub.status.idle":"2023-05-17T16:41:11.804324Z","shell.execute_reply.started":"2023-05-17T16:41:11.788583Z","shell.execute_reply":"2023-05-17T16:41:11.803281Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Female    7032\nMale      5130\nName: Gender, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"### Handling Class Imbalance","metadata":{}},{"cell_type":"code","source":"# Removing classes with very less emotions captured\ndata_path_rem=data_path[(data_path['Emotions']!='surprise')&(data_path['Emotions']!='calm')]","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:15.502495Z","iopub.execute_input":"2023-05-17T16:41:15.502902Z","iopub.status.idle":"2023-05-17T16:41:15.515115Z","shell.execute_reply.started":"2023-05-17T16:41:15.502867Z","shell.execute_reply":"2023-05-17T16:41:15.513927Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation\n\n- Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set.\n- To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed.\n- The objective is to make our model invariant to those perturbations and enhace its ability to generalize.","metadata":{}},{"cell_type":"code","source":"rate=0.8","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:39.866385Z","iopub.execute_input":"2023-05-17T16:41:39.867176Z","iopub.status.idle":"2023-05-17T16:41:39.872138Z","shell.execute_reply.started":"2023-05-17T16:41:39.867136Z","shell.execute_reply":"2023-05-17T16:41:39.870780Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"sampling_rate=22050\ndef noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data):\n    return librosa.effects.time_stretch(data, rate=2.0)\n\ndef pitch(data):\n    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=10)\n\nimport scipy.signal as signal\n\ndef low_pass_filter(data):\n    # Define the filter parameters\n    nyquist_freq = 0.5 * sampling_rate\n    cutoff_freq = 5000  # Set the cutoff frequency of the filter to 5 kHz\n    numtaps = 10        # Set the number of filter taps (length of the filter)\n\n    # Create a low-pass filter with the specified parameters\n    taps = signal.firwin(numtaps, cutoff_freq/nyquist_freq)\n\n    # Apply the filter to the audio signal\n    filtered_audio = signal.convolve(data, taps, mode='same')\n\n    return filtered_audio","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:40.298562Z","iopub.execute_input":"2023-05-17T16:41:40.299717Z","iopub.status.idle":"2023-05-17T16:41:40.306546Z","shell.execute_reply.started":"2023-05-17T16:41:40.299656Z","shell.execute_reply":"2023-05-17T16:41:40.305761Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Feature Extraction - Statistical and accoustic \n\nExtracting 5 features:\n- Zero Crossing Rate\n- Chroma_stft\n- MFCC\n- RMS(root mean square) value\n- MelSpectogram to train our model.\n\nThe statistical features that are extracted are:\n- Mean\n- Variance\n- Skewness\n- Kurtosis\n- Audio_rms\n- Spectral centroids\n- Spectral bandwidth","metadata":{}},{"cell_type":"code","source":"sample_rate=22050\nimport os, glob\nimport librosa\nimport numpy as np\nimport scipy\ndef extract_features(data):\n    # ZCR\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result=np.hstack((result, zcr)) # stacking horizontally\n\n    # Chroma_stft\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, chroma_stft)) # stacking horizontally\n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mfcc)) # stacking horizontally\n\n    # Root Mean Square Value\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    result = np.hstack((result, rms)) # stacking horizontally\n\n    # MelSpectogram\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mel)) # stacking horizontally\n    \n    return result\n\ndef statistical_features(data):\n    result = np.array([])\n    mean = np.mean(data)#\n    result=np.hstack((result, mean)) # stacking horizontally\n    \n    variance = np.var(data)#\n    result=np.hstack((result, variance)) # stacking horizontally\n    \n    skewness = scipy.stats.skew(data)#\n    result=np.hstack((result, skewness)) # stacking horizontally\n    \n    kurtosis = scipy.stats.kurtosis(data)#\n    result=np.hstack((result, kurtosis)) # stacking horizontally\n    \n    audio_rms = librosa.feature.rms(y=data)\n    audio_rms_mean = audio_rms.mean()#\n    result=np.hstack((result, audio_rms_mean)) # stacking horizontally\n    \n    spectral_centroids = librosa.feature.spectral_centroid(y=data, sr=sampling_rate)[0]#\n    result=np.hstack((result, spectral_centroids.mean())) # stacking horizontally\n    \n    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=data, sr=sampling_rate)[0]#\n    result=np.hstack((result, spectral_bandwidth.mean())) # stacking horizontally\n    \n    mfccs = librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13)\n    mfcc = mfccs.mean()#\n    result=np.hstack((result, mfcc)) # stacking horizontally\n    \n    return result\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n\n    # without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    stat=statistical_features(data)\n    result=np.hstack((result,stat))\n    \n    # data with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    stat2=statistical_features(noise_data)\n    result2=np.hstack((res2,stat2))\n    result1 = np.vstack((result, result2)) # stacking vertically\n    \n    # data with stretch\n    stretched_data = stretch(data)\n    res3 = extract_features(stretched_data)\n    stat3=statistical_features(stretched_data)\n    result2=np.hstack((res3,stat3))\n    \n    # data with LPF\n    low_pass_filter_data = low_pass_filter(data)\n    res4 = extract_features(low_pass_filter_data)\n    stat4=statistical_features(low_pass_filter_data)\n    result3=np.hstack((res4,stat4))\n    \n    # data with pitch\n    pitch_data = pitch(data)\n    res5 = extract_features(pitch_data)\n    stat5=statistical_features(pitch_data)\n    result4=np.hstack((res5,stat5))\n\n    result = np.vstack((result, result2,result3,result4)) # stacking vertically\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:41:41.114136Z","iopub.execute_input":"2023-05-17T16:41:41.115118Z","iopub.status.idle":"2023-05-17T16:41:41.133227Z","shell.execute_reply.started":"2023-05-17T16:41:41.115079Z","shell.execute_reply":"2023-05-17T16:41:41.131941Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Creating a sequence of features and target","metadata":{}},{"cell_type":"code","source":"X_all, Y,gender_list_all = [], [],[]\ncount = 1\nfor path, emotion, gender in zip(data_path_rem.Path, data_path_rem.Emotions,data_path_rem.Gender):\n    feature = get_features(path)\n    for ele in feature:\n        X_all.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)\n        gender_list_all.append(gender)\n    if (count % 500) == 0:\n        print('Done Reading '+str(count)+'/'+str(len(data_path_rem))+' files')\n    count+=1","metadata":{"execution":{"iopub.status.busy":"2023-05-17T04:49:16.241083Z","iopub.execute_input":"2023-05-17T04:49:16.241447Z","iopub.status.idle":"2023-05-17T05:40:48.877804Z","shell.execute_reply.started":"2023-05-17T04:49:16.241422Z","shell.execute_reply":"2023-05-17T05:40:48.877075Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"500/11318\n1000/11318\n1500/11318\n2000/11318\n2500/11318\n3000/11318\n3500/11318\n4000/11318\n4500/11318\n5000/11318\n5500/11318\n6000/11318\n6500/11318\n7000/11318\n7500/11318\n8000/11318\n8500/11318\n9000/11318\n9500/11318\n10000/11318\n10500/11318\n11000/11318\n","output_type":"stream"}]},{"cell_type":"code","source":"X_all = np.load('/kaggle/input/features/X_all.npy')\nY = np.load('/kaggle/input/features/Y_all.npy')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:19.581230Z","iopub.execute_input":"2023-05-17T18:55:19.582493Z","iopub.status.idle":"2023-05-17T18:55:19.687876Z","shell.execute_reply.started":"2023-05-17T18:55:19.582449Z","shell.execute_reply":"2023-05-17T18:55:19.686672Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"actors = data_path_rem['Actor'].to_list()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:22.720723Z","iopub.execute_input":"2023-05-17T18:55:22.722004Z","iopub.status.idle":"2023-05-17T18:55:22.728866Z","shell.execute_reply.started":"2023-05-17T18:55:22.721953Z","shell.execute_reply":"2023-05-17T18:55:22.727530Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"### Extracting Gender Features","metadata":{}},{"cell_type":"code","source":"# For the train and validation set\ngender=[]\nfor i in gender_list_all:\n    if i=='Male':\n        gender.append(1)\n    else:\n        gender.append(0)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:56:26.062883Z","iopub.execute_input":"2023-05-17T16:56:26.063367Z","iopub.status.idle":"2023-05-17T16:56:26.083732Z","shell.execute_reply.started":"2023-05-17T16:56:26.063325Z","shell.execute_reply":"2023-05-17T16:56:26.081758Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"X_gender = np.expand_dims(np.stack(gender), -1)\nX = np.concatenate([X_all, X_gender], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:29.631782Z","iopub.execute_input":"2023-05-17T18:55:29.632162Z","iopub.status.idle":"2023-05-17T18:55:29.719359Z","shell.execute_reply.started":"2023-05-17T18:55:29.632132Z","shell.execute_reply":"2023-05-17T18:55:29.718294Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation\n\n- As of now we have extracted the data, now we need to normalize and split our data for training and testing.","metadata":{}},{"cell_type":"code","source":"def preprocess_data_and_labels(X, Y):\n    encoder = OneHotEncoder()\n    Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n    \n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    \n    return X,Y\n\ndef split_acoustic_statistical_gender(X):\n    return X[:, :162], X[:, 162:170], X[:,-1]","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:33.080655Z","iopub.execute_input":"2023-05-17T18:55:33.081090Z","iopub.status.idle":"2023-05-17T18:55:33.087943Z","shell.execute_reply.started":"2023-05-17T18:55:33.081052Z","shell.execute_reply":"2023-05-17T18:55:33.086807Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"### Deep Feature Extraction","metadata":{}},{"cell_type":"code","source":"num_classes=6","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:34.120760Z","iopub.execute_input":"2023-05-17T18:55:34.121675Z","iopub.status.idle":"2023-05-17T18:55:34.126481Z","shell.execute_reply.started":"2023-05-17T18:55:34.121635Z","shell.execute_reply":"2023-05-17T18:55:34.125278Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"def get_feature_extraction_model(input_shape, num_classes):\n    model=Sequential()\n    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=input_shape))\n    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\n    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n    model.add(Dropout(0.2))\n\n    model.add(Flatten())\n    model.add(Dense(units=32, activation='relu'))\n    model.add(Dropout(0.2))\n\n    model.add(Dense(units=12, activation='relu'))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(units=num_classes, activation='softmax'))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:34.491179Z","iopub.execute_input":"2023-05-17T18:55:34.491555Z","iopub.status.idle":"2023-05-17T18:55:34.499462Z","shell.execute_reply.started":"2023-05-17T18:55:34.491524Z","shell.execute_reply":"2023-05-17T18:55:34.498649Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def fit_evaluate_return_FE_model(model, X_train, y_train, X_test, y_test):\n    model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=8)\n    rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=6, min_lr=0.0000001)\n    callback_list=[rlrp,early_stopping_callback]\n    \n    model.fit(X_train, y_train, batch_size=128, epochs=5, validation_data=(X_test, y_test), callbacks=callback_list, verbose=0)\n    # model.evaluate(X_test,y_test)\n    \n    conv1d_feature_model=Model(inputs=model.input,outputs=model.layers[-5].output)\n    return conv1d_feature_model","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:32:34.770634Z","iopub.execute_input":"2023-05-17T19:32:34.771352Z","iopub.status.idle":"2023-05-17T19:32:34.782231Z","shell.execute_reply.started":"2023-05-17T19:32:34.771309Z","shell.execute_reply":"2023-05-17T19:32:34.780818Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"### MLP - Dense Network Classifier","metadata":{}},{"cell_type":"code","source":"def get_classification_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Dense(12, input_shape=input_shape, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:38.430794Z","iopub.execute_input":"2023-05-17T18:55:38.431180Z","iopub.status.idle":"2023-05-17T18:55:38.436724Z","shell.execute_reply.started":"2023-05-17T18:55:38.431150Z","shell.execute_reply":"2023-05-17T18:55:38.435558Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"## Actor-wise Cross Validation","metadata":{}},{"cell_type":"code","source":"import random\n\nrandom.seed(42)  # Set the random seed for reproducibility\n\nactor_list = data_path_rem['Actor'].unique().tolist()\nrandom.shuffle(actor_list)\n\nactor_folds = []\ntrain_folds = []\ntest_fold = []\n\nfor i in range(0, 11):\n    actor_folds.append(actor_list[(11 * i):(11 * i) + 11])\n    all_folds = list(range(11))\n    all_folds.remove(i)\n    train_folds.append(all_folds)\n    test_fold.append(i)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:41.252727Z","iopub.execute_input":"2023-05-17T18:55:41.253330Z","iopub.status.idle":"2023-05-17T18:55:41.259760Z","shell.execute_reply.started":"2023-05-17T18:55:41.253296Z","shell.execute_reply":"2023-05-17T18:55:41.258899Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"print(actor_folds)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:24:34.739767Z","iopub.execute_input":"2023-05-17T20:24:34.740289Z","iopub.status.idle":"2023-05-17T20:24:34.748453Z","shell.execute_reply.started":"2023-05-17T20:24:34.740254Z","shell.execute_reply":"2023-05-17T20:24:34.747099Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"[[28, 100, 78, 102, 92, 37, 54, 62, 77, 98, 57], [64, 67, 74, 27, 112, 40, 87, 84, 45, 33, 11], [52, 115, 31, 81, 14, 75, 58, 83, 12, 10, 9], [44, 96, 73, 109, 43, 51, 103, 59, 36, 86, 23], [25, 60, 4, 55, 110, 79, 13, 120, 18, 22, 121], [20, 104, 117, 90, 108, 41, 116, 68, 35, 114, 50], [47, 29, 42, 82, 107, 48, 113, 19, 2, 16, 72], [1, 8, 56, 49, 61, 65, 21, 32, 30, 6, 5], [15, 71, 39, 111, 106, 38, 119, 118, 91, 80, 101], [24, 66, 63, 7, 26, 95, 89, 69, 76, 105, 97], [94, 46, 34, 70, 53, 3, 99, 85, 88, 93, 17]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_folds)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:24:52.228061Z","iopub.execute_input":"2023-05-17T20:24:52.228469Z","iopub.status.idle":"2023-05-17T20:24:52.233914Z","shell.execute_reply.started":"2023-05-17T20:24:52.228437Z","shell.execute_reply":"2023-05-17T20:24:52.232911Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 3, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 4, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 5, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 8, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 9, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 10], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(test_fold)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:25:16.928114Z","iopub.execute_input":"2023-05-17T20:25:16.928525Z","iopub.status.idle":"2023-05-17T20:25:16.935019Z","shell.execute_reply.started":"2023-05-17T20:25:16.928492Z","shell.execute_reply":"2023-05-17T20:25:16.933109Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_indices_of_actors(actors_list, list_of_actors):\n    bool_list = [True if actor in list_of_actors else False for actor in actors_list]\n    return [i for i,x in enumerate(bool_list) if x==True]","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:44.800609Z","iopub.execute_input":"2023-05-17T18:55:44.801383Z","iopub.status.idle":"2023-05-17T18:55:44.808180Z","shell.execute_reply.started":"2023-05-17T18:55:44.801345Z","shell.execute_reply":"2023-05-17T18:55:44.806796Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"num_classes = 6\ndef actor_wise_cross_validation(X, Y, actors):\n    X, Y = preprocess_data_and_labels(X, Y)\n    X_ac, X_stat, X_gender = split_acoustic_statistical_gender(X)\n    \n    for i in range(len(train_folds)):\n        print('Fold '+str(i+1)+'/'+str(len(train_folds)))\n        \n        # Get the indices for Train and Test Folds\n        train_fold_actors = []\n        for j in train_folds[i]:\n            train_fold_actors.extend(actor_folds[j])\n        test_fold_actors = actor_folds[i]\n        train_actor_indices = get_indices_of_actors(actors, train_fold_actors)\n        test_actor_indices = get_indices_of_actors(actors, test_fold_actors)\n        \n        # Separate acoustic features array for deep feature extraction\n        X_ac_train = X_ac[train_actor_indices]\n        X_ac_val = X_ac[test_actor_indices]\n        X_ac_train = np.expand_dims(X_ac_train, -1)\n        X_ac_val = np.expand_dims(X_ac_val, -1)\n        \n        # Seperate statistical and gender features\n        X_stat_train = X_stat[train_actor_indices]\n        X_stat_val = X_stat[test_actor_indices]\n        X_gender_train = np.expand_dims(X_gender[train_actor_indices], -1)\n        X_gender_val = np.expand_dims(X_gender[test_actor_indices], -1)\n        y_train = Y[train_actor_indices]\n        y_val = Y[test_actor_indices]\n        \n        # Get deep feature extraction model\n        df_model = get_feature_extraction_model((X_ac_train.shape[1], 1), num_classes)\n        \n        # Train the model\n        print(\"Extracting Deep features\")\n        conv1d_feature_model = fit_evaluate_return_FE_model(df_model, X_ac_train, y_train, X_ac_val, y_val)\n        \n        # Transform the acoustic features\n        df_train = conv1d_feature_model.predict(X_ac_train)\n        df_val = conv1d_feature_model.predict(X_ac_val)\n        \n        X_train = np.concatenate([df_train, X_stat_train, X_gender_train], axis=1)\n        X_val = np.concatenate([df_val, X_stat_val, X_gender_val], axis=1)\n        \n        # Create Classification model\n        model = get_classification_model((X_train.shape[1],), num_classes)\n        print(\"Training Model\")\n        model.compile(optimizer = 'adam' , \n                       loss = 'categorical_crossentropy', \n                       metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n        model.fit(X_train,y_train,validation_data=(X_val, y_val),epochs=50,batch_size=64, verbose=0)\n        model.evaluate(X_val, y_val)\n        \n        print('Done')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:34:25.134992Z","iopub.execute_input":"2023-05-17T19:34:25.135524Z","iopub.status.idle":"2023-05-17T19:34:25.155038Z","shell.execute_reply.started":"2023-05-17T19:34:25.135487Z","shell.execute_reply":"2023-05-17T19:34:25.153528Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"actor_wise_cross_validation(X, Y, actors)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:34:25.718914Z","iopub.execute_input":"2023-05-17T19:34:25.720552Z","iopub.status.idle":"2023-05-17T20:10:56.167425Z","shell.execute_reply.started":"2023-05-17T19:34:25.720488Z","shell.execute_reply":"2023-05-17T20:10:56.165985Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Fold 1/11\nExtracting Deep features\n325/325 [==============================] - 7s 21ms/step\n29/29 [==============================] - 1s 21ms/step\nTraining Model\n29/29 [==============================] - 0s 2ms/step - loss: 1.4694 - accuracy: 0.4043 - precision_51: 0.5508 - recall_51: 0.1816\nDone\nFold 2/11\nExtracting Deep features\n326/326 [==============================] - 7s 20ms/step\n28/28 [==============================] - 1s 20ms/step\nTraining Model\n28/28 [==============================] - 0s 2ms/step - loss: 1.3778 - accuracy: 0.4318 - precision_53: 0.6421 - recall_53: 0.2063\nDone\nFold 3/11\nExtracting Deep features\n331/331 [==============================] - 7s 20ms/step\n24/24 [==============================] - 0s 19ms/step\nTraining Model\n24/24 [==============================] - 0s 2ms/step - loss: 1.3243 - accuracy: 0.4573 - precision_55: 0.6498 - recall_55: 0.2053\nDone\nFold 4/11\nExtracting Deep features\n327/327 [==============================] - 7s 20ms/step\n27/27 [==============================] - 1s 20ms/step\nTraining Model\n27/27 [==============================] - 0s 3ms/step - loss: 1.2907 - accuracy: 0.4630 - precision_57: 0.6350 - recall_57: 0.2477\nDone\nFold 5/11\nExtracting Deep features\n296/296 [==============================] - 6s 22ms/step\n59/59 [==============================] - 1s 23ms/step\nTraining Model\n59/59 [==============================] - 0s 2ms/step - loss: 1.4068 - accuracy: 0.4304 - precision_59: 0.6392 - recall_59: 0.1868\nDone\nFold 6/11\nExtracting Deep features\n327/327 [==============================] - 7s 20ms/step\n27/27 [==============================] - 1s 20ms/step\nTraining Model\n27/27 [==============================] - 0s 3ms/step - loss: 1.3947 - accuracy: 0.4167 - precision_61: 0.6310 - recall_61: 0.1840\nDone\nFold 7/11\nExtracting Deep features\n329/329 [==============================] - 7s 20ms/step\n26/26 [==============================] - 1s 20ms/step\nTraining Model\n26/26 [==============================] - 0s 2ms/step - loss: 1.3566 - accuracy: 0.4525 - precision_63: 0.7222 - recall_63: 0.2084\nDone\nFold 8/11\nExtracting Deep features\n331/331 [==============================] - 7s 21ms/step\n23/23 [==============================] - 0s 19ms/step\nTraining Model\n23/23 [==============================] - 0s 2ms/step - loss: 1.3664 - accuracy: 0.4533 - precision_65: 0.7308 - recall_65: 0.2088\nDone\nFold 9/11\nExtracting Deep features\n328/328 [==============================] - 7s 22ms/step\n27/27 [==============================] - 1s 21ms/step\nTraining Model\n27/27 [==============================] - 0s 2ms/step - loss: 1.4091 - accuracy: 0.4101 - precision_67: 0.6166 - recall_67: 0.1833\nDone\nFold 10/11\nExtracting Deep features\n293/293 [==============================] - 6s 19ms/step\n61/61 [==============================] - 1s 19ms/step\nTraining Model\n61/61 [==============================] - 0s 2ms/step - loss: 1.4370 - accuracy: 0.4316 - precision_69: 0.5961 - recall_69: 0.1723\nDone\nFold 11/11\nExtracting Deep features\n328/328 [==============================] - 7s 20ms/step\n26/26 [==============================] - 1s 22ms/step\nTraining Model\n26/26 [==============================] - 0s 2ms/step - loss: 1.3566 - accuracy: 0.4504 - precision_71: 0.6234 - recall_71: 0.1804\nDone\n","output_type":"stream"}]}]}