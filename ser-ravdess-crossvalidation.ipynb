{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  <center> Speech Emotion Recognition (Ravdess Dataset)<center>\n##   <center> (With Cross Validation) <center>","metadata":{}},{"cell_type":"markdown","source":"### Importing Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-05-17T20:15:12.113765Z","iopub.execute_input":"2023-05-17T20:15:12.114169Z","iopub.status.idle":"2023-05-17T20:15:23.208433Z","shell.execute_reply.started":"2023-05-17T20:15:12.114137Z","shell.execute_reply":"2023-05-17T20:15:23.206817Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##  <center> 1. Ravdess Dataframe <center>\nThe filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).","metadata":{}},{"cell_type":"markdown","source":"### Read Ravdess audio data","metadata":{}},{"cell_type":"code","source":"# Paths for data.\nRavdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:15:23.210958Z","iopub.execute_input":"2023-05-17T20:15:23.211815Z","iopub.status.idle":"2023-05-17T20:15:23.217349Z","shell.execute_reply.started":"2023-05-17T20:15:23.211778Z","shell.execute_reply":"2023-05-17T20:15:23.215933Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\n\nfile_emotion = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(Ravdess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nRavdess_df = pd.concat([emotion_df, path_df], axis=1)\n\n# changing integers to actual emotions.\nRavdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df['Actor'] = Ravdess_df['Path'].str.split('-').str[-1].str.split('.').str[0]\nRavdess_df['Actor'] = Ravdess_df['Actor'].apply(lambda x: int(x))\nRavdess_df['Gender'] = np.where(Ravdess_df['Actor'].astype(int) % 2 == 1, 'Male', 'Female')\n\n\npd.set_option('display.max_colwidth', None)\nRavdess_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:15:28.762524Z","iopub.execute_input":"2023-05-17T20:15:28.762968Z","iopub.status.idle":"2023-05-17T20:15:29.244059Z","shell.execute_reply.started":"2023-05-17T20:15:28.762938Z","shell.execute_reply":"2023-05-17T20:15:29.243241Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Emotions  \\\n0  surprise   \n1   neutral   \n2   disgust   \n3   disgust   \n4   neutral   \n\n                                                                                                       Path  \\\n0  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-08-01-01-01-02.wav   \n1  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-01-01-02.wav   \n2  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-02-01-02-02.wav   \n3  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-01-01-02-02.wav   \n4  /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-02-01-02.wav   \n\n   Actor  Gender  \n0      2  Female  \n1      2  Female  \n2      2  Female  \n3      2  Female  \n4      2  Female  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotions</th>\n      <th>Path</th>\n      <th>Actor</th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>surprise</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-08-01-01-01-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-01-01-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-02-01-02-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disgust</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-07-01-01-02-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-01-01-02-01-02.wav</td>\n      <td>2</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Class Imbalance","metadata":{}},{"cell_type":"code","source":"# Removing classes with very less emotions captured\ndata_path_rem=Ravdess_df[(Ravdess_df['Emotions']!='surprise')&(Ravdess_df['Emotions']!='calm')]","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:16:57.062256Z","iopub.execute_input":"2023-05-17T20:16:57.062695Z","iopub.status.idle":"2023-05-17T20:16:57.073588Z","shell.execute_reply.started":"2023-05-17T20:16:57.062657Z","shell.execute_reply":"2023-05-17T20:16:57.071828Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation\n\n- Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set.\n- To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed.","metadata":{}},{"cell_type":"code","source":"rate=0.8","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:17:23.235653Z","iopub.execute_input":"2023-05-17T20:17:23.236091Z","iopub.status.idle":"2023-05-17T20:17:23.241590Z","shell.execute_reply.started":"2023-05-17T20:17:23.236056Z","shell.execute_reply":"2023-05-17T20:17:23.240521Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"sampling_rate=22050\ndef noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data):\n    return librosa.effects.time_stretch(data, rate=2.0)\n\ndef pitch(data):\n    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=10)\n\nimport scipy.signal as signal\n\ndef low_pass_filter(data):\n    # Define the filter parameters\n    nyquist_freq = 0.5 * sampling_rate\n    cutoff_freq = 5000  # Set the cutoff frequency of the filter to 5 kHz\n    numtaps = 10        # Set the number of filter taps (length of the filter)\n\n    # Create a low-pass filter with the specified parameters\n    taps = signal.firwin(numtaps, cutoff_freq/nyquist_freq)\n\n    # Apply the filter to the audio signal\n    filtered_audio = signal.convolve(data, taps, mode='same')\n\n    return filtered_audio","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:17:27.372626Z","iopub.execute_input":"2023-05-17T20:17:27.373211Z","iopub.status.idle":"2023-05-17T20:17:27.482128Z","shell.execute_reply.started":"2023-05-17T20:17:27.373161Z","shell.execute_reply":"2023-05-17T20:17:27.480372Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Feature Extraction - Statistical and accoustic \nExtracting 5 features:\n- Zero Crossing Rate\n- Chroma_stft\n- MFCC\n- RMS(root mean square) value\n- MelSpectogram to train our model.\n\nThe statistical features that are extracted are:\n- Mean\n- Variance\n- Skewness\n- Kurtosis\n- Audio_rms\n- Spectral centroids\n- Spectral bandwidth","metadata":{}},{"cell_type":"code","source":"sample_rate=22050\nimport os, glob\nimport librosa\nimport numpy as np\nimport scipy\ndef extract_features(data):\n    # ZCR\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result=np.hstack((result, zcr)) # stacking horizontally\n\n    # Chroma_stft\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, chroma_stft)) # stacking horizontally\n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mfcc)) # stacking horizontally\n\n    # Root Mean Square Value\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    result = np.hstack((result, rms)) # stacking horizontally\n\n    # MelSpectogram\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mel)) # stacking horizontally\n    \n    return result\n\ndef statistical_features(data):\n    result = np.array([])\n    mean = np.mean(data)#\n    result=np.hstack((result, mean)) # stacking horizontally\n    \n    variance = np.var(data)#\n    result=np.hstack((result, variance)) # stacking horizontally\n    \n    skewness = scipy.stats.skew(data)#\n    result=np.hstack((result, skewness)) # stacking horizontally\n    \n    kurtosis = scipy.stats.kurtosis(data)#\n    result=np.hstack((result, kurtosis)) # stacking horizontally\n    \n    audio_rms = librosa.feature.rms(y=data)\n    audio_rms_mean = audio_rms.mean()#\n    result=np.hstack((result, audio_rms_mean)) # stacking horizontally\n    \n    spectral_centroids = librosa.feature.spectral_centroid(y=data, sr=sampling_rate)[0]#\n    result=np.hstack((result, spectral_centroids.mean())) # stacking horizontally\n    \n    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=data, sr=sampling_rate)[0]#\n    result=np.hstack((result, spectral_bandwidth.mean())) # stacking horizontally\n    \n    mfccs = librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13)\n    mfcc = mfccs.mean()#\n    result=np.hstack((result, mfcc)) # stacking horizontally\n    \n    return result\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n\n    # without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    stat=statistical_features(data)\n    result=np.hstack((result,stat))\n    \n    # data with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    stat2=statistical_features(noise_data)\n    result2=np.hstack((res2,stat2))\n    result1 = np.vstack((result, result2)) # stacking vertically\n    \n    # data with stretch\n    stretched_data = stretch(data)\n    res3 = extract_features(stretched_data)\n    stat3=statistical_features(stretched_data)\n    result2=np.hstack((res3,stat3))\n    \n    # data with LPF\n    low_pass_filter_data = low_pass_filter(data)\n    res4 = extract_features(low_pass_filter_data)\n    stat4=statistical_features(low_pass_filter_data)\n    result3=np.hstack((res4,stat4))\n    \n    # data with pitch\n    pitch_data = pitch(data)\n    res5 = extract_features(pitch_data)\n    stat5=statistical_features(pitch_data)\n    result4=np.hstack((res5,stat5))\n\n    result = np.vstack((result, result2,result3,result4)) # stacking vertically\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:17:40.333858Z","iopub.execute_input":"2023-05-17T20:17:40.334820Z","iopub.status.idle":"2023-05-17T20:17:40.356619Z","shell.execute_reply.started":"2023-05-17T20:17:40.334765Z","shell.execute_reply":"2023-05-17T20:17:40.354705Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Creating a sequence of features and target","metadata":{}},{"cell_type":"code","source":"X_all, Y,gender_list_all = [], [],[]\ncount = 1\nfor path, emotion, gender in zip(data_path_rem.Path, data_path_rem.Emotions,data_path_rem.Gender):\n    feature = get_features(path)\n    for ele in feature:\n        X_all.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)\n        gender_list_all.append(gender)\n    if (count % 500) == 0:\n        print('Done Reading '+str(count)+'/'+str(len(data_path_rem))+' files')\n    count+=1","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:18:33.582548Z","iopub.execute_input":"2023-05-17T20:18:33.583004Z","iopub.status.idle":"2023-05-17T20:26:41.476839Z","shell.execute_reply.started":"2023-05-17T20:18:33.582973Z","shell.execute_reply":"2023-05-17T20:26:41.476025Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Done Reading 500/1056 files\nDone Reading 1000/1056 files\n","output_type":"stream"}]},{"cell_type":"code","source":"actors = data_path_rem['Actor'].to_list()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:27:00.034012Z","iopub.execute_input":"2023-05-17T20:27:00.034457Z","iopub.status.idle":"2023-05-17T20:27:00.040164Z","shell.execute_reply.started":"2023-05-17T20:27:00.034425Z","shell.execute_reply":"2023-05-17T20:27:00.039022Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Gender Features","metadata":{}},{"cell_type":"code","source":"# For the train and validation set\ngender=[]\nfor i in gender_list_all:\n    if i=='Male':\n        gender.append(1)\n    else:\n        gender.append(0)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:27:05.165671Z","iopub.execute_input":"2023-05-17T20:27:05.166107Z","iopub.status.idle":"2023-05-17T20:27:05.173072Z","shell.execute_reply.started":"2023-05-17T20:27:05.166073Z","shell.execute_reply":"2023-05-17T20:27:05.171390Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"X_gender = np.expand_dims(np.stack(gender), -1)\nX = np.concatenate([X_all, X_gender], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:27:23.482871Z","iopub.execute_input":"2023-05-17T20:27:23.484364Z","iopub.status.idle":"2023-05-17T20:27:23.501885Z","shell.execute_reply.started":"2023-05-17T20:27:23.484312Z","shell.execute_reply":"2023-05-17T20:27:23.499584Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"def preprocess_data_and_labels(X, Y):\n    encoder = OneHotEncoder()\n    Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n    \n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    \n    return X,Y\n\ndef split_acoustic_statistical_gender(X):\n    return X[:, :162], X[:, 162:170], X[:,-1]","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:27:36.842909Z","iopub.execute_input":"2023-05-17T20:27:36.843324Z","iopub.status.idle":"2023-05-17T20:27:36.849983Z","shell.execute_reply.started":"2023-05-17T20:27:36.843290Z","shell.execute_reply":"2023-05-17T20:27:36.848504Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Deep Feature Extraction","metadata":{}},{"cell_type":"code","source":"num_classes=6","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:27:42.351741Z","iopub.execute_input":"2023-05-17T20:27:42.352124Z","iopub.status.idle":"2023-05-17T20:27:42.358856Z","shell.execute_reply.started":"2023-05-17T20:27:42.352090Z","shell.execute_reply":"2023-05-17T20:27:42.356549Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def get_feature_extraction_model(input_shape, num_classes):\n    model=Sequential()\n    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=input_shape))\n    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\n    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n    model.add(Dropout(0.2))\n\n    model.add(Flatten())\n    model.add(Dense(units=32, activation='relu'))\n    model.add(Dropout(0.2))\n\n    model.add(Dense(units=12, activation='relu'))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(units=num_classes, activation='softmax'))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:27:51.772524Z","iopub.execute_input":"2023-05-17T20:27:51.772915Z","iopub.status.idle":"2023-05-17T20:27:51.781700Z","shell.execute_reply.started":"2023-05-17T20:27:51.772881Z","shell.execute_reply":"2023-05-17T20:27:51.780497Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def fit_evaluate_return_FE_model(model, X_train, y_train, X_test, y_test):\n    model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=8)\n    rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=6, min_lr=0.0000001)\n    callback_list=[rlrp,early_stopping_callback]\n    \n    model.fit(X_train, y_train, batch_size=128, epochs=5, validation_data=(X_test, y_test), callbacks=callback_list, verbose=0)\n    # model.evaluate(X_test,y_test)\n    \n    conv1d_feature_model=Model(inputs=model.input,outputs=model.layers[-5].output)\n    return conv1d_feature_model","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:27:56.977217Z","iopub.execute_input":"2023-05-17T20:27:56.977608Z","iopub.status.idle":"2023-05-17T20:27:56.985262Z","shell.execute_reply.started":"2023-05-17T20:27:56.977576Z","shell.execute_reply":"2023-05-17T20:27:56.983956Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### MLP - Dense Network Classifier","metadata":{}},{"cell_type":"code","source":"def get_classification_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Dense(12, input_shape=input_shape, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:27:58.292201Z","iopub.execute_input":"2023-05-17T20:27:58.294836Z","iopub.status.idle":"2023-05-17T20:27:58.302223Z","shell.execute_reply.started":"2023-05-17T20:27:58.294771Z","shell.execute_reply":"2023-05-17T20:27:58.301089Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Actor-wise Cross Validation","metadata":{}},{"cell_type":"markdown","source":"### Folds split based on actors","metadata":{}},{"cell_type":"code","source":"import random\nactor_folds = []\ntrain_folds = []\ntest_fold = []\nfor i in range(6):\n    actor_folds.append(list(range((4*i)+1, (4*i)+5)))\n    all_folds = list(range(6))\n    all_folds.remove(i)\n    train_folds.append(all_folds)\n    test_fold.append(i)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:28:00.777820Z","iopub.execute_input":"2023-05-17T20:28:00.779563Z","iopub.status.idle":"2023-05-17T20:28:00.786140Z","shell.execute_reply.started":"2023-05-17T20:28:00.779504Z","shell.execute_reply":"2023-05-17T20:28:00.784400Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def get_indices_of_actors(actors_list, list_of_actors):\n    bool_list = [True if actor in list_of_actors else False for actor in actors_list]\n    return [i for i,x in enumerate(bool_list) if x==True]","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:28:02.792508Z","iopub.execute_input":"2023-05-17T20:28:02.792868Z","iopub.status.idle":"2023-05-17T20:28:02.800225Z","shell.execute_reply.started":"2023-05-17T20:28:02.792837Z","shell.execute_reply":"2023-05-17T20:28:02.798604Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"num_classes = 6\ndef actor_wise_cross_validation(X, Y, actors):\n    X, Y = preprocess_data_and_labels(X, Y)\n    X_ac, X_stat, X_gender = split_acoustic_statistical_gender(X)\n    \n    for i in range(len(train_folds)):\n        print('Fold '+str(i+1)+'/'+str(len(train_folds)))\n        \n        # Get the indices for Train and Test Folds\n        train_fold_actors = []\n        for j in train_folds[i]:\n            train_fold_actors.extend(actor_folds[j])\n        test_fold_actors = actor_folds[i]\n        train_actor_indices = get_indices_of_actors(actors, train_fold_actors)\n        test_actor_indices = get_indices_of_actors(actors, test_fold_actors)\n        \n        # Separate acoustic features array for deep feature extraction\n        X_ac_train = X_ac[train_actor_indices]\n        X_ac_val = X_ac[test_actor_indices]\n        X_ac_train = np.expand_dims(X_ac_train, -1)\n        X_ac_val = np.expand_dims(X_ac_val, -1)\n        \n        # Seperate statistical and gender features\n        X_stat_train = X_stat[train_actor_indices]\n        X_stat_val = X_stat[test_actor_indices]\n        X_gender_train = np.expand_dims(X_gender[train_actor_indices], -1)\n        X_gender_val = np.expand_dims(X_gender[test_actor_indices], -1)\n        y_train = Y[train_actor_indices]\n        y_val = Y[test_actor_indices]\n        \n        # Get deep feature extraction model\n        df_model = get_feature_extraction_model((X_ac_train.shape[1], 1), num_classes)\n        \n        # Train the model\n        print(\"Extracting Deep features\")\n        conv1d_feature_model = fit_evaluate_return_FE_model(df_model, X_ac_train, y_train, X_ac_val, y_val)\n        \n        # Transform the acoustic features\n        df_train = conv1d_feature_model.predict(X_ac_train)\n        df_val = conv1d_feature_model.predict(X_ac_val)\n        \n        X_train = np.concatenate([df_train, X_stat_train, X_gender_train], axis=1)\n        X_val = np.concatenate([df_val, X_stat_val, X_gender_val], axis=1)\n        \n        # Create Classification model\n        model = get_classification_model((X_train.shape[1],), num_classes)\n        print(\"Training Model\")\n        model.compile(optimizer = 'adam' , \n                       loss = 'categorical_crossentropy', \n                       metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n        model.fit(X_train,y_train,validation_data=(X_val, y_val),epochs=50,batch_size=64, verbose=0)\n        model.evaluate(X_val, y_val)\n        \n        print('Done')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:28:15.349750Z","iopub.execute_input":"2023-05-17T20:28:15.350295Z","iopub.status.idle":"2023-05-17T20:28:15.364589Z","shell.execute_reply.started":"2023-05-17T20:28:15.350248Z","shell.execute_reply":"2023-05-17T20:28:15.362821Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"actor_wise_cross_validation(X, Y, actors)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:29:05.022119Z","iopub.execute_input":"2023-05-17T20:29:05.022587Z","iopub.status.idle":"2023-05-17T20:31:32.189460Z","shell.execute_reply.started":"2023-05-17T20:29:05.022548Z","shell.execute_reply":"2023-05-17T20:31:32.188142Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Fold 1/6\nExtracting Deep features\n28/28 [==============================] - 1s 23ms/step\n6/6 [==============================] - 0s 21ms/step\nTraining Model\n6/6 [==============================] - 0s 5ms/step - loss: 1.4837 - accuracy: 0.3920 - precision_1: 0.6038 - recall_1: 0.1818\nDone\nFold 2/6\nExtracting Deep features\n28/28 [==============================] - 1s 22ms/step\n6/6 [==============================] - 0s 23ms/step\nTraining Model\n6/6 [==============================] - 0s 3ms/step - loss: 1.3093 - accuracy: 0.5000 - precision_3: 0.7755 - recall_3: 0.2159\nDone\nFold 3/6\nExtracting Deep features\n28/28 [==============================] - 1s 18ms/step\n6/6 [==============================] - 0s 19ms/step\nTraining Model\n6/6 [==============================] - 0s 3ms/step - loss: 1.3293 - accuracy: 0.4659 - precision_5: 0.8723 - recall_5: 0.2330\nDone\nFold 4/6\nExtracting Deep features\n28/28 [==============================] - 1s 19ms/step\n6/6 [==============================] - 0s 18ms/step\nTraining Model\n6/6 [==============================] - 0s 3ms/step - loss: 1.3908 - accuracy: 0.4205 - precision_7: 0.6441 - recall_7: 0.2159\nDone\nFold 5/6\nExtracting Deep features\n28/28 [==============================] - 1s 19ms/step\n6/6 [==============================] - 0s 19ms/step\nTraining Model\n6/6 [==============================] - 0s 3ms/step - loss: 1.4747 - accuracy: 0.4034 - precision_9: 0.6379 - recall_9: 0.2102\nDone\nFold 6/6\nExtracting Deep features\n28/28 [==============================] - 1s 21ms/step\n6/6 [==============================] - 0s 18ms/step\nTraining Model\n6/6 [==============================] - 0s 3ms/step - loss: 1.4318 - accuracy: 0.3920 - precision_11: 0.5846 - recall_11: 0.2159\nDone\n","output_type":"stream"}]}]}